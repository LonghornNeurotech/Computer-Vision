{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7ff3318",
      "metadata": {
        "id": "e7ff3318"
      },
      "source": [
        "\n",
        "# LunarLander RL Template: Q-Learning and DQN introduction\n",
        "\n",
        "This notebook provides a reinforcement learning scaffold for **LunarLander-v3** (discrete) in **Gymnasium**.\n",
        "\n",
        "You will be introduced to 2 different approaches:\n",
        "- **A) Discretized Tabular Q-Learning**: bins the continuous state into discrete buckets, then learns a dictionary-based Q-table.\n",
        "- **B) DQN (Deep Q-Network)**: uses your **MLP** to approximate the Q-function, with experience replay and a target network.\n",
        "\n",
        "You will notice one will perform much better than the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ec90ed",
      "metadata": {
        "id": "42ec90ed"
      },
      "outputs": [],
      "source": [
        "#@title Environment setup for Colab\n",
        "# This cell sets up all dependencies for the LunarLander notebook on Google Colab.\n",
        "import sys, subprocess, os\n",
        "\n",
        "def pipi(*args):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
        "\n",
        "# Upgrade base tools\n",
        "pipi(\"--upgrade\", \"pip\", \"setuptools\", \"wheel\")\n",
        "\n",
        "# Gymnasium + Box2D\n",
        "try:\n",
        "    pipi(\"gymnasium[box2d]\")\n",
        "    try:\n",
        "        pipi(\"box2d-py\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"[install] box2d-py failed; using Box2D fallback.\")\n",
        "        pipi(\"Box2D\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"[install] gymnasium[box2d] failed; trying gymnasium and Box2D separately.\")\n",
        "    pipi(\"gymnasium\")\n",
        "    pipi(\"Box2D\")\n",
        "\n",
        "# Rendering / video / DL\n",
        "pipi(\"pygame\", \"imageio\", \"imageio-ffmpeg\", \"matplotlib\", \"torch\")\n",
        "\n",
        "print(\"Setup complete. Continue below.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a3bfb7",
      "metadata": {
        "id": "04a3bfb7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, glob, random, math, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Optional, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import Video, display\n",
        "\n",
        "# ==== Global Configuration ====\n",
        "ENV_ID = \"LunarLander-v3\"       # Discrete environment\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# On Colab, store videos in /content for easy preview\n",
        "VIDEO_DIR = \"/content/videos\"\n",
        "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
        "\n",
        "MAX_STEPS = 2500\n",
        "RNG = np.random.default_rng(SEED)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some helper functions"
      ],
      "metadata": {
        "id": "tYDQka0i_I8m"
      },
      "id": "tYDQka0i_I8m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a0cec7",
      "metadata": {
        "id": "e3a0cec7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_env(env_id: str = ENV_ID, seed: int = SEED, render_mode: Optional[str] = None):\n",
        "    \"\"\"Create and seed a Gymnasium environment with a safe fallback.\"\"\"\n",
        "    try:\n",
        "        env = gym.make(env_id, render_mode=render_mode)\n",
        "    except Exception as e:\n",
        "        print(f\"[make_env] Could not create '{env_id}' ({e}). Falling back to 'LunarLander-v3'.\")\n",
        "        env = gym.make(\"LunarLander-v3\", render_mode=render_mode)\n",
        "\n",
        "    try:\n",
        "        env.reset(seed=seed)\n",
        "    except TypeError:\n",
        "        env.reset()\n",
        "    try:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return env\n",
        "\n",
        "\n",
        "def rollout_episode(env, policy: Callable, max_steps: int = MAX_STEPS, capture_frames: bool = False):\n",
        "    \"\"\"Run a single episode with the provided policy callable: action = policy(obs, action_space).\"\"\"\n",
        "    frames = []\n",
        "    obs, info = env.reset(seed=SEED)\n",
        "    total_reward = 0.0\n",
        "    for t in range(max_steps):\n",
        "        if capture_frames and hasattr(env, \"render\"):\n",
        "            frame = env.render()\n",
        "            if frame is not None:\n",
        "                frames.append(frame)\n",
        "        action = policy(obs, env.action_space)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        total_reward += float(reward)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_reward, frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e5d0eb",
      "metadata": {
        "id": "30e5d0eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import deque\n",
        "\n",
        "@dataclass\n",
        "class RewardTracker:\n",
        "    window: int = 100\n",
        "    def __post_init__(self):\n",
        "        self.history = []\n",
        "        self._dq = deque(maxlen=self.window)\n",
        "    def update(self, ret: float):\n",
        "        self.history.append(ret)\n",
        "        self._dq.append(ret)\n",
        "    @property\n",
        "    def moving_avg(self) -> float:\n",
        "        return float(np.mean(self._dq)) if self._dq else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d8e344",
      "metadata": {
        "id": "c6d8e344"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_policy(obs, action_space):\n",
        "    return action_space.sample()\n",
        "\n",
        "def record_and_display(policy_fn: Callable, env_id: str = ENV_ID, seed: int = SEED, video_dir: str = VIDEO_DIR, max_steps: int = MAX_STEPS):\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "    env = make_env(env_id, seed=seed, render_mode=\"rgb_array\")\n",
        "    env = RecordVideo(env, video_dir, episode_trigger=lambda e: True, name_prefix=\"demo\")\n",
        "    total, _ = rollout_episode(env, policy_fn, max_steps=max_steps, capture_frames=False)\n",
        "    env.close()\n",
        "\n",
        "    mp4s = sorted(glob.glob(os.path.join(video_dir, \"*.mp4\")))\n",
        "    latest = mp4s[-1] if mp4s else None\n",
        "    print(f\"Episode return: {total:.2f}\")\n",
        "    if latest:\n",
        "        display(Video(latest, embed=True, html_attributes=\"controls loop autoplay\"))\n",
        "    else:\n",
        "        print(\"No video found. If on Colab, ensure imageio-ffmpeg is installed.\")\n",
        "    return total, latest\n",
        "\n",
        "# Example video of a random policy\n",
        "_ = record_and_display(random_policy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(agent, obs, action_space, epsilon: float = 0.0):\n",
        "  return agent.select_action(obs, action_space, epsilon)\n",
        "\n",
        "def to_tensor(obs: np.ndarray) -> torch.Tensor:\n",
        "    return torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)"
      ],
      "metadata": {
        "id": "R3r6LfpODV0K"
      },
      "id": "R3r6LfpODV0K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64086dc0"
      },
      "outputs": [],
      "source": [
        "def record_agent_video(agent: nn.Module, env_id: str = ENV_ID, seed: int = SEED,\n",
        "                       video_dir: str = VIDEO_DIR, max_steps: int = MAX_STEPS):\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "    env = make_env(env_id, seed=seed, render_mode=\"rgb_array\")\n",
        "    env = RecordVideo(env, video_dir, episode_trigger=lambda e: True, name_prefix=\"dqn\")\n",
        "    total = 0.0\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    for _ in range(max_steps):\n",
        "        obs = to_tensor(obs)\n",
        "        a = select_action(agent, obs, env.action_space, epsilon=0.0)\n",
        "        obs, r, term, trunc, _ = env.step(a)\n",
        "        total += float(r)\n",
        "        if term or trunc:\n",
        "            break\n",
        "    env.close()\n",
        "    mp4s = sorted(glob.glob(os.path.join(video_dir, \"*.mp4\")))\n",
        "    latest = mp4s[-1] if mp4s else None\n",
        "    print(f\"Agent video return: {total:.2f}\")\n",
        "    if latest:\n",
        "        display(Video(latest, embed=True, html_attributes=\"controls loop autoplay\"))\n",
        "    return total, latest\n"
      ],
      "id": "64086dc0"
    },
    {
      "cell_type": "markdown",
      "id": "0b72022d",
      "metadata": {
        "id": "0b72022d"
      },
      "source": [
        "\n",
        "## A) Tabular Q-Learning (with state discretization)\n",
        "\n",
        "LunarLander observations are continuous. We **bin** each dimension into a small number of buckets to get a discrete state key. Then we apply vanilla Q-learning:\n",
        "```\n",
        "Q[s,a] ← Q[s,a] + α (r + γ max_a' Q[s',a'] − Q[s,a])\n",
        "```\n",
        "Good for learning dynamics, but **DQN**s are preferred for function approximation and scalability.\n",
        "\n",
        "See what results you can get with the tabular Q-learning approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discretizer:\n",
        "    \"\"\"Uniformly discretize each observation dimension into `bins` buckets.\"\"\"\n",
        "    def __init__(self, low: np.ndarray, high: np.ndarray, bins: int = 8):\n",
        "        self.bins = bins\n",
        "        low = np.where(np.isfinite(low), low, -1.0)\n",
        "        high = np.where(np.isfinite(high), high, 1.0)\n",
        "        self.low = low\n",
        "        self.high = high\n",
        "\n",
        "    def encode(self, obs: np.ndarray) -> Tuple[int, ...]:\n",
        "        ratios = (obs - self.low) / (self.high - self.low + 1e-8)\n",
        "        ratios = np.clip(ratios, 0.0, 1.0)\n",
        "        idxs = (ratios * self.bins).astype(int)\n",
        "        idxs = np.clip(idxs, 0, self.bins - 1)\n",
        "        return tuple(int(i) for i in idxs)"
      ],
      "metadata": {
        "id": "BZ2vj7vdFuQ3"
      },
      "id": "BZ2vj7vdFuQ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class LunarLanderAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        discount_factor: float = 0.95,\n",
        "    ):\n",
        "        \"\"\"Initialize a Q-Learning agent.\n",
        "\n",
        "        Args:\n",
        "            env: The training environment\n",
        "            learning_rate: How quickly to update Q-values (0-1)\n",
        "            initial_epsilon: Starting exploration rate (usually 1.0)\n",
        "            epsilon_decay: How much to reduce epsilon each episode\n",
        "            final_epsilon: Minimum exploration rate (usually 0.1)\n",
        "            discount_factor: How much to value future rewards (0-1)\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "\n",
        "        self.disc = Discretizer(env.observation_space.low, env.observation_space.high)\n",
        "\n",
        "        # Q-table: maps (state, action) to expected reward\n",
        "        # defaultdict automatically creates entries with zeros for new states\n",
        "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.discount_factor = discount_factor  # How much we care about future rewards\n",
        "\n",
        "        # Exploration parameters\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "        # Track learning progress\n",
        "        self.training_error = []\n",
        "\n",
        "    def select_action(self, obs: tuple[int, int, bool], action_space = None, epsilon = None) -> int:\n",
        "        \"\"\"Choose an action using epsilon-greedy strategy.\n",
        "\n",
        "        Returns:\n",
        "            action: 0, 1, 2, or 3 (left, right, up, down)\n",
        "        \"\"\"\n",
        "        #TODO: implement the epsilon-greedy strategy\n",
        "        comp = np.random.random()\n",
        "        if (comp < epsilon):\n",
        "          choice = np.random.choice(action_space)\n",
        "        else:\n",
        "          choice = np.argmax(self.q_values[obs])\n",
        "        action_decision = choice\n",
        "\n",
        "        return action_decision\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: tuple[int, int, bool],\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple[int, int, bool],\n",
        "    ):\n",
        "        \"\"\"Update Q-value based on experience.\n",
        "\n",
        "        This is the heart of Q-learning: learn from (state, action, reward, next_state)\n",
        "        \"\"\"\n",
        "        obs = self.disc.encode(obs)\n",
        "        next_obs = self.disc.encode(next_obs)\n",
        "\n",
        "        # What should the Q-value be? (Bellman equation, defined as V(s) = max_a(R(s, a) + g*V(s')))\n",
        "        # a simplified version would be V(s) = R(s, a) + max(g*V(s')), which is basically saying that\n",
        "        # the expected value is equal the reward of the current state and action + the max/best q-value possible for the next state * some discount factor\n",
        "\n",
        "        # What's the best we could do from the next state?\n",
        "        # (Zero if episode terminated - no future rewards possible)\n",
        "        best_next_q = max(self.q_values[next_obs])  #TODO: calculate the best q-value possible for the next state that will be used in the Bellman equation\n",
        "\n",
        "        target = reward + (best_next_q * self.discount_factor) #TODO: calculate target value using the Bellman equation\n",
        "\n",
        "        # How wrong was our current estimate?\n",
        "        temporal_difference = target - self.q_values[obs][action]\n",
        "\n",
        "        # Update our estimate in the direction of the error\n",
        "        # Learning rate controls how big steps we take\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "\n",
        "        # Track learning progress (useful for debugging)\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
        "        self.epsilon = self.epsilon * self.epsilon_decay # TODO: implement epsilon decaying"
      ],
      "metadata": {
        "id": "OVtt-3J3_Ja4"
      },
      "id": "OVtt-3J3_Ja4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_q_agent(agent, n_episodes):\n",
        "  for episode in tqdm(range(n_episodes)):\n",
        "      # Start a new landing\n",
        "      obs, info = env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "          # Agent chooses action (initially random, gradually more intelligent)\n",
        "          action = agent.select_action(obs)\n",
        "\n",
        "          # Take action and observe result\n",
        "          next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "          # Learn from this experience\n",
        "          agent.update(obs, action, reward, terminated, next_obs)\n",
        "\n",
        "          # Move to next state\n",
        "          done = terminated or truncated\n",
        "          obs = next_obs\n",
        "\n",
        "      if episode % 1000 == 0:\n",
        "        print(\"Reward:\", reward)\n",
        "\n",
        "      # Reduce exploration rate (agent becomes less random over time)\n",
        "      agent.decay_epsilon()"
      ],
      "metadata": {
        "id": "IwGqRyiqGAvD"
      },
      "id": "IwGqRyiqGAvD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "learning_rate = 0.01        # @param How fast to learn (higher = faster but less stable)\n",
        "n_episodes = 50_000        # @param Number of landings to practice (may need to increase this)\n",
        "start_epsilon = 1.0         # @param Start with 100% random actions\n",
        "epsilon_decay = 0.997 # @param Reduce exploration over time\n",
        "min_epsilon = 0.1         # @param Always keep some exploration\n",
        "\n",
        "# Create environment and agent\n",
        "env = make_env(ENV_ID, SEED, render_mode=None)\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
        "\n",
        "q_agent = LunarLanderAgent(\n",
        "    env=env,\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=min_epsilon,\n",
        ")"
      ],
      "metadata": {
        "id": "xKkakayh_ZxV"
      },
      "id": "xKkakayh_ZxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_q_agent(q_agent, n_episodes)"
      ],
      "metadata": {
        "id": "Fle5Eww9_j3X"
      },
      "id": "Fle5Eww9_j3X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize your tabular Q-learning lunar lander\n"
      ],
      "metadata": {
        "id": "iPU9mHalFa5C"
      },
      "id": "iPU9mHalFa5C"
    },
    {
      "cell_type": "code",
      "source": [
        "record_agent_video(q_agent)"
      ],
      "metadata": {
        "id": "kmBhbI_ZDz-I"
      },
      "id": "kmBhbI_ZDz-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "71676201",
      "metadata": {
        "id": "71676201"
      },
      "source": [
        "\n",
        "## B) Double DQN (Deep Q-Network)\n",
        "\n",
        "We train a \"policy\" Q-network (`DQNModel`) with:\n",
        "- **Experience Replay** buffer\n",
        "- **Target Network** (periodically updated)\n",
        "  - this separate network is used to calculate target Q-values, which decouples action selection (done by the policy network) and action evaluation\n",
        "  - typically less prone to overestimating Q-values than standard single network DQN\n",
        "- **ε-greedy** exploration\n",
        "- **Huber loss** and **Adam** optimizer\n",
        "\n",
        "Complete any TODOs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "918938a7"
      },
      "outputs": [],
      "source": [
        "class DQNModel(nn.Module):\n",
        "    \"\"\"Q-network architecture.\n",
        "\n",
        "    For LunarLander:\n",
        "      - Input: observation vector shape [8]\n",
        "      - Output: Q-values for each action (shape [n_actions], 4)\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_dim: int, n_actions: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        # TODO: implement architecture\n",
        "        # hint: very simple MLP is all you need\n",
        "        self.linear1 = nn.Linear(obs_dim, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, hidden)\n",
        "        self.linear3 = nn.Linear(hidden, n_actions)\n",
        "        self.act_fn = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      #TODO: return the model output (the Q-value)\n",
        "      x = self.linear1(x)\n",
        "      x = self.act_fn(x)\n",
        "      x = self.linear2(x)\n",
        "      x = self.act_fn(x)\n",
        "      return self.linear3(x)\n",
        "\n",
        "    def select_action(self, obs: np.ndarray, action_space, epsilon: float = 0.0) -> int:\n",
        "      \"\"\"Map model outputs to a valid discrete action.\n",
        "\n",
        "      - With probability epsilon, choose a random action.\n",
        "      - Otherwise, choose argmax Q-value from the model.\n",
        "      \"\"\"\n",
        "      # TODO: return an action (int in {0,1,2,3})\n",
        "      num = np.random.random()\n",
        "      if (num < epsilon):\n",
        "        choice = np.sample(action_space)\n",
        "      else:\n",
        "        choice = forward(obs).argmax().item() # If it does not work, try adding dim = (+/-)1 argument to .argmax()\n",
        "      # hint: similar to how you implemented epsilon-greedy in tabular q-learning, but how do you find the max q-value from the model?\n",
        "      # think about what the network represents or approximates\n",
        "      raise NotImplementedError(\"implement epsilon-greedy over model Q-values here.\")\n",
        "\n"
      ],
      "id": "918938a7"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple(\n",
        "    \"Transition\", [\"state\", \"action\", \"next_state\", \"reward\", \"done\"]\n",
        ")\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return (\n",
        "            random.sample(self.memory, batch_size)\n",
        "            if batch_size < len(self.memory)\n",
        "            else self.memory\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "SJXLKiGRUnLK"
      },
      "id": "SJXLKiGRUnLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0604ee",
      "metadata": {
        "id": "4b0604ee"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DQNConfig:\n",
        "  num_episodes: int = 500 # @param\n",
        "  gamma: float = 0.99 # @param\n",
        "  learning_rate: float = 1e-4 # @param\n",
        "  tau: float = 0.005 # @param\n",
        "  batch_size: int = 128 # @param\n",
        "  epsilon: float = 1.0 # @param\n",
        "  epsilon_decay: float = 0.995 # @param\n",
        "  epsilon_min: float = 0.01 # @param\n",
        "  eval_interval: int = 100 # @param\n",
        "  eval_episodes: int = 5 # @param\n",
        "  max_grad_norm: float = 10.0 # @param\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3dd6b3",
      "metadata": {
        "id": "0e3dd6b3"
      },
      "outputs": [],
      "source": [
        "# def linear_epsilon(step: int, start: float, end: float, decay_steps: int) -> float:\n",
        "#     if step >= decay_steps:\n",
        "#         return end\n",
        "#     return start + (end - start) * (step / float(decay_steps))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model: nn.Module, episodes: int = 5, env_id: str = ENV_ID, seed: int = SEED):\n",
        "    scores = []\n",
        "    for ep in range(episodes):\n",
        "        env = make_env(env_id, seed=seed + ep, render_mode=None)\n",
        "        total = 0.0\n",
        "        obs, _ = env.reset(seed=seed + ep)\n",
        "        for _ in range(MAX_STEPS):\n",
        "            obs = to_tensor(obs)\n",
        "            a = select_action(model, obs, env.action_space, epsilon=0.0)\n",
        "            obs, r, term, trunc, _ = env.step(a)\n",
        "            total += float(r)\n",
        "            if term or trunc:\n",
        "                break\n",
        "        env.close()\n",
        "        scores.append(total)\n",
        "    mean_ret = float(np.mean(scores))\n",
        "    print(f\"Eval over {episodes} episodes — mean return: {mean_ret:.2f}\")\n",
        "    return mean_ret\n",
        "\n",
        "def dqn_train(cfg: DQNConfig):\n",
        "    # Initialize the environment\n",
        "    env = gym.make(ENV_ID, render_mode=\"human\")\n",
        "\n",
        "    n_observations = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    policy_net = DQNModel(n_observations, n_actions).to(DEVICE)\n",
        "    target_net = DQNModel(n_observations, n_actions).to(DEVICE)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    replay_memory = ReplayMemory(10_000)\n",
        "\n",
        "    optimizer = optim.AdamW(policy_net.parameters(), lr=cfg.learning_rate)\n",
        "    # smooth l1 loss is implementation of Huber loss (MSE for small errors, L1 for larger errors)\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "\n",
        "    epsilon = cfg.epsilon\n",
        "\n",
        "    best_reward = float(\"-inf\")\n",
        "\n",
        "    for episode in tqdm(range(cfg.num_episodes)):\n",
        "      state, _ = env.reset(seed=SEED)\n",
        "      state = to_tensor(state)\n",
        "      tracker = RewardTracker()\n",
        "      total_reward = 0.0\n",
        "\n",
        "      while True:\n",
        "        action = select_action(policy_net, state, env.action_space, epsilon)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        reward = torch.tensor([reward], device=DEVICE)\n",
        "        next_state = to_tensor(next_state)\n",
        "        replay_memory.push(state, torch.tensor([[action]], device=DEVICE), next_state, reward, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += 0 #TODO: fix the total_reward update line\n",
        "\n",
        "        if len(replay_memory) >= cfg.batch_size:\n",
        "            transitions = replay_memory.sample(cfg.batch_size)\n",
        "            states, actions, next_states, rewards, dones = zip(*transitions)\n",
        "\n",
        "            states_batch = torch.cat(states)\n",
        "            next_states_batch = torch.cat(next_states)\n",
        "            actions_batch = torch.cat(actions)\n",
        "            rewards = torch.tensor(rewards, device=DEVICE)\n",
        "            dones = torch.tensor(dones, device=DEVICE)\n",
        "\n",
        "            # target network calculates target q-values (action evaluation)\n",
        "            q_target = (\n",
        "                cfg.gamma * target_net(next_states_batch).detach().max(-1)[0] * ~dones\n",
        "                + rewards\n",
        "            )\n",
        "\n",
        "            #policy network determines action selection\n",
        "            q_policy = policy_net(states_batch).gather(1, actions_batch)\n",
        "\n",
        "            # Calculate the Huber loss (remember that the Huber loss behaves like MSE for errors < q_target and L1 for errors > q_target)\n",
        "            loss = None  #TODO: use the criterion defined above\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # In-place gradient clipping to stabilize training\n",
        "            # TODO: use gradient norm clipping on the policy network; see torch.nn.utils.clip_grad_norm_()\n",
        "            # hint: there is a config member named `max_grad_norm`\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update target network (target network is updated less frequently, and it is updated by copying weights from the policy network)\n",
        "        for target_param, main_param in zip(target_net.parameters(), policy_net.parameters()):\n",
        "            target_param.data.copy_(cfg.tau * main_param.data + (1 - cfg.tau) * target_param.data)\n",
        "\n",
        "        if done:\n",
        "            tracker.update(total_reward)\n",
        "            if total_reward > best_reward:\n",
        "                best_reward = total_reward\n",
        "                torch.save(policy_net.state_dict(), \"best_policy.pth\")\n",
        "            if episode % 25 == 0:\n",
        "                print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "            if episode % cfg.eval_interval == 0:\n",
        "                evaluate_model(policy_net, episodes=cfg.eval_episodes)\n",
        "            break\n",
        "\n",
        "      epsilon = epsilon # TODO: decay the epsilon\n",
        "\n",
        "    env.close()\n",
        "    return policy_net, target_net, replay_memory, tracker\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# default config should work well, but you are encouraged to try different hyperparams;\n",
        "# e.g., increasing the number of episodes or experimenting with decay rate\n",
        "cfg = DQNConfig()"
      ],
      "metadata": {
        "id": "n_1_BPZwm41c"
      },
      "id": "n_1_BPZwm41c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f01ea4",
      "metadata": {
        "id": "51f01ea4"
      },
      "outputs": [],
      "source": [
        "# will raise NotImplementedError until you implement the TODOs in DQNModel\n",
        "policy_net, target_net, rb, tracker = dqn_train(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_policy_net_state = torch.load(\"best_policy.pth\")\n",
        "best_policy_net = DQNModel(8, 4).to(DEVICE)\n",
        "best_policy_net.load_state_dict(best_policy_net_state)"
      ],
      "metadata": {
        "id": "9ezRM_XfCraO"
      },
      "id": "9ezRM_XfCraO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c20e8f76",
      "metadata": {
        "id": "c20e8f76"
      },
      "source": [
        "### Visualize your lunar lander\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f283f45",
      "metadata": {
        "id": "3f283f45"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(f\"Final moving average: {tr.moving_avg:.2f}\")\n",
        "record_agent_video(best_policy_net)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
